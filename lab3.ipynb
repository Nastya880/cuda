{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMymz12jgFRbjnfiUzrXDqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nastya880/cuda/blob/main/lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Лабораторная 3**"
      ],
      "metadata": {
        "id": "nHEKBKXxllKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Редукция"
      ],
      "metadata": {
        "id": "09KrS1rnlwJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "#include <stdio.h>\n",
        "#include <stdint.h>\n",
        "#include <assert.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <iostream>\n",
        "#include <time.h>\n",
        "#include <random>\n",
        "\n",
        "const int BLOCK_SIZE = 16;\n",
        "const int GRID_SIZE = 16;\n",
        "\n",
        "const int arr_length = BLOCK_SIZE*GRID_SIZE;\n",
        "\n",
        "const dim3 threads(BLOCK_SIZE, 1);\n",
        "const dim3 grid(GRID_SIZE, 1);\n",
        "\n",
        "//__global__ void reduction(float * inData, float * outData)\n",
        "//{\n",
        "//\t__shared__ int data[BLOCK_SIZE];\n",
        "//\tint tid = threadIdx.x;\n",
        "//\tint i = 2 * blockIdx.x * blockDim.x + threadIdx.x;\n",
        "//\tdata[tid] = inData[i] + inData[i + blockDim.x];\n",
        "//\t__syncthreads();\n",
        "//\tfor (int s = blockDim.x / 2; s > 0; s >>= 1)\n",
        "//\t{\n",
        "//\t\tif (tid < s)\n",
        "//\t\t\tdata[tid] += data[tid + s];\n",
        "//\t\t__syncthreads();\n",
        "//\t}\n",
        "//\tif (tid == 0)\n",
        "//\t\toutData[blockIdx.x] = data[0];\n",
        "//}\n",
        "\n",
        "__global__ void reduction(float * inData, float * outData)\n",
        "{\n",
        "\t__shared__ int data[BLOCK_SIZE];\n",
        "\tint tid = threadIdx.x;\n",
        "\tint i = 2 * blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tdata[tid] = inData[i] < inData[i + blockDim.x]? inData[i] : inData[i + blockDim.x];\n",
        "\t__syncthreads();\n",
        "\tfor (int s = blockDim.x / 2; s > 0; s >>= 1)\n",
        "\t{\n",
        "\t\tif (tid < s)\n",
        "\t\t\tdata[tid] = data[tid] < data[tid + s] ? data[tid] : data[tid + s];\n",
        "\t\t__syncthreads();\n",
        "\t}\n",
        "\tif (tid == 0)\n",
        "\t\toutData[blockIdx.x] = data[0];\n",
        "}\n",
        "\n",
        "void generateRandom(float *arr) {\n",
        "\tfor (int i = 0; i < arr_length; i++)\n",
        "\t{\n",
        "\t\t//arr[i] = 1;\n",
        "\t\tarr[i] = rand() % 40 + 10;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void showArray(float *arr)\n",
        "{\n",
        "\tfor (int i = 0; i < arr_length; i++)\n",
        "\t{\n",
        "\t\tprintf_s(\"%.0f \", arr[i]);\n",
        "\t}\n",
        "\tprintf_s(\"\\n\\n\");\n",
        "}\n",
        "\n",
        "void checkCudaErrors(cudaError_t err)\n",
        "{\n",
        "\tif (err != cudaSuccess)\n",
        "\t{\n",
        "\t\tprintf(\"%s\\n\", cudaGetErrorString(err));\n",
        "\t\tsystem(\"pause\");\n",
        "\t}\n",
        "}\n",
        "\n",
        "void cpu_reduction(float* arr, int size) {\n",
        "\tdouble time = 0;\n",
        "\tfloat min = 999999;\n",
        "\tclock_t start = clock();\n",
        "\tfor (int i = 0; i < size; i++)\n",
        "\t\tif (min > arr[i])\n",
        "\t\t\tmin = arr[i];\n",
        "\tclock_t end = clock();\n",
        "\tprintf(\"CPU result %.2f\\n\", min);\n",
        "\tprintf(\"CPU time: %.5f ms\\n\", (double)(end - start) / CLOCKS_PER_SEC * 1000);\n",
        "}\n",
        "\n",
        "// Start the main CUDA Sample here\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tint memorySize = sizeof(float) * arr_length;\n",
        "\n",
        "\t//Выделение памяти на хосте\n",
        "\tfloat *host_A = (float*)malloc(memorySize);\n",
        "\tfloat *host_C = (float*)malloc(memorySize);\n",
        "\n",
        "\t//Выделение памяти на девайсе\n",
        "\tcheckCudaErrors(cudaSetDevice(0));\n",
        "\tfloat *device_A;\n",
        "\tfloat *device_C;\n",
        "\tcheckCudaErrors(cudaMalloc((void**)&device_A, memorySize));\n",
        "\tcheckCudaErrors(cudaMalloc((void**)&device_C, memorySize));\n",
        "\n",
        "\n",
        "\t//Инициализация данных и отображение\n",
        "\tgenerateRandom(host_A);\n",
        "\tshowArray(host_A);\n",
        "\n",
        "\t//Копирование данных на девайс\n",
        "\tcheckCudaErrors(cudaMemcpy(device_A, host_A, memorySize, cudaMemcpyHostToDevice));\n",
        "\n",
        "\n",
        "\t//Настройка и запуск ядра\n",
        "\tcudaEvent_t start, stop;\n",
        "\tfloat gpuTime = 0.0f;\n",
        "\tcheckCudaErrors(cudaEventCreate(&start));\n",
        "\tcheckCudaErrors(cudaEventCreate(&stop));\n",
        "\tcheckCudaErrors(cudaEventRecord(start, 0));\n",
        "\n",
        "\treduction <<<grid, threads >>> (device_A, device_C);\n",
        "\n",
        "\tcheckCudaErrors(cudaEventRecord(stop, 0));\n",
        "\tcheckCudaErrors(cudaEventSynchronize(stop));\n",
        "\tcheckCudaErrors(cudaEventElapsedTime(&gpuTime, start, stop));\n",
        "\tcheckCudaErrors(cudaEventDestroy(start));\n",
        "\tcheckCudaErrors(cudaEventDestroy(stop));\n",
        "\n",
        "\t//Копирование данных с девайса\n",
        "\tcheckCudaErrors(cudaMemcpy(host_C, device_C, memorySize, cudaMemcpyDeviceToHost));\n",
        "\n",
        "\t//Проверка результатов и освобождение памяти\n",
        "\tshowArray(host_C);\n",
        "\tprintf(\"GPU time: %.5f ms\\n\", gpuTime);\n",
        "\tfloat result = 99999;\n",
        "\tfor (int i = 0; i < arr_length; i++)\n",
        "\t\tif (result > host_C[i] && host_C[i] > 0)\n",
        "\t\t\tresult = host_C[i];\n",
        "\tprintf(\"GPU result: %.2f \\n\", result);\n",
        "\tcpu_reduction(host_A, arr_length);\n",
        "\n",
        "\tcudaDeviceReset();\n",
        "\tcudaFree(device_A);\n",
        "\tcudaFree(device_C);\n",
        "\tfree(host_A);\n",
        "\tfree(host_C);\n",
        "\n",
        "\tsystem(\"pause\");\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "50kKGnd2lvH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -Wno-deprecated-gpu-targets\n",
        "!nvprof ./reduction"
      ],
      "metadata": {
        "id": "qMtj9-Wml3-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "гистограмма "
      ],
      "metadata": {
        "id": "Hn47pOq3mNB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#include <stdio.h>\n",
        "#include <stdint.h>\n",
        "#include <assert.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <iostream>\n",
        "#include <time.h>\n",
        "#include <random>\n",
        "\n",
        "typedef unsigned int uint;\n",
        "typedef unsigned char uchar;\n",
        "\n",
        "#define N\t\t\t\t(6*1024*1024)\t//Размер массива\n",
        "\n",
        "#define LOG2_WARP_SIZE  5\t\t\t\t// логарифм размера warp's по основанию 2\n",
        "#define WARP_SIZE       32\t\t\t\t// Размер warp'а\n",
        "#define TAG_MASK\t\t0x07FFFFFFU\t\t//Маска для снятия идентификатора нити\n",
        "#define NUM_BINS\t\t256\t\t\t\t// число счетчиков в гистограмме \n",
        "#define NUM_WARPS       6\t\t\t\t// Число warp'ов в блоке \n",
        "#define MERGE_THREADBLOCK_SIZE\t256\n",
        "inline __device__ void addByte(volatile uint * warpHist, uint data, uint threadTag)\n",
        "{\n",
        "\tuint count;\n",
        "\tdo {\n",
        "\t\t// прочесть текущее значение счетчика и снять идентификатор нити \n",
        "\t\tcount = warpHist[data] & TAG_MASK;\n",
        "\t\t\n",
        "\t\t// увеличить его на единицу и поставить свой идентификатор\n",
        "\t\tcount = threadTag | (count + 1);\n",
        "\n",
        "\t\t//осуществить запись\n",
        "\t\twarpHist[data] = count;\n",
        "\t}\n",
        "\twhile (warpHist[data] != count); // проверить, прошла ли запись\n",
        "}\n",
        "\n",
        "inline __device__ void addWord(volatile uint *warpHist, uint data, uint tag)\n",
        "{\n",
        "\taddByte(warpHist, (data >> 0) & 0xFFU, tag);\n",
        "\taddByte(warpHist, (data >> 8) & 0xFFU, tag);\n",
        "\taddByte(warpHist, (data >> 16) & 0xFFU, tag);\n",
        "\taddByte(warpHist, (data >> 24) & 0xFFU, tag);\n",
        "}\n",
        "\n",
        "__global__ void histogramKernel(uint * partialHistograms, uint * data, uint dataCount)\n",
        "{\n",
        "\t//Своя гистограмма на каждый варп\n",
        "\t__shared__ uint hist[NUM_BINS*NUM_WARPS];\n",
        "\tuint *warpHist = hist + (threadIdx.x >> LOG2_WARP_SIZE) * NUM_BINS;\n",
        "\n",
        "\t// очистить счетчики гистограмм\n",
        "#pragma unroll\n",
        "\tfor (uint i = 0; i < NUM_BINS / WARP_SIZE; i++)\n",
        "\t\thist[threadIdx.x + i*NUM_WARPS*WARP_SIZE] = 0;\n",
        "\n",
        "\t// получить id для данной нити\n",
        "\tuint tag = threadIdx.x << (32 - LOG2_WARP_SIZE);\n",
        "\n",
        "\t__syncthreads();\n",
        "\n",
        "\t//Построить гистограммы по заданному набору элементов\n",
        "\tfor (uint pos = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\tpos < dataCount;\n",
        "\t\tpos += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t\tuint d = data[pos];\n",
        "\t\taddWord(warpHist, d, tag);\n",
        "\t}\n",
        "\t__syncthreads();\n",
        "\n",
        "\t// объединить гистограммы данного блока и записать результат в глобальную память\n",
        "\t// 192 нити суммируют данные до 256 элементов гистограммы\n",
        "\tfor (uint bin = threadIdx.x; bin < NUM_BINS; bin += (NUM_WARPS*WARP_SIZE))\n",
        "\t{\n",
        "\t\tuint sum = 0;\n",
        "\t\tfor (uint i = 0; i < NUM_WARPS; i++)\n",
        "\t\t\tsum += hist[bin + i*NUM_BINS] & TAG_MASK;\n",
        "\n",
        "\t\tpartialHistograms[blockIdx.x * NUM_BINS + bin] = sum;\n",
        "\t}\n",
        "}\n",
        "\n",
        "// объединить гистограммы, один блок на каждый NUM_BINS элементов\n",
        "__global__ void mergeHistogramKernel(uint * outHistogram, uint * partialHistograms, uint histogramCount)\n",
        "{\n",
        "\tuint sum = 0;\n",
        "\tfor (uint i = threadIdx.x; i < histogramCount; i += 256)\n",
        "\t\tsum += partialHistograms[blockIdx.x + i * NUM_BINS];\n",
        "\n",
        "\t__shared__ uint data[NUM_BINS];\n",
        "\n",
        "\tdata[threadIdx.x] = sum;\n",
        "\n",
        "\tfor (uint stride = NUM_BINS / 2; stride > 0; stride >>= 1)\n",
        "\t{\n",
        "\t\t__syncthreads();\n",
        "\n",
        "\t\tif (threadIdx.x < stride)\n",
        "\t\t\tdata[threadIdx.x] += data[threadIdx.x + stride];\n",
        "\t}\n",
        "\n",
        "\tif (threadIdx.x == 0) \n",
        "\t\toutHistogram[blockIdx.x] = data[0];\n",
        "}\n",
        "\n",
        "void histogram(uint * histogram, void * dataDev, uint byteCount)\n",
        "{\n",
        "\tassert(byteCount % 4 == 0);\n",
        "\n",
        "\tint n = byteCount / 4;\n",
        "\tint numBlocks = n / (NUM_WARPS*WARP_SIZE);\n",
        "\tint numPartials = 240;\n",
        "\tuint *partialHistograms = NULL;\n",
        "\n",
        "\t//выделить память под гистограммы блоков\n",
        "\tcudaMalloc((void**)&partialHistograms, numPartials*NUM_BINS * sizeof(uint));\n",
        "\t\n",
        "\t// построить гистограмму для каждого блока\n",
        "\thistogramKernel <<<dim3(numPartials), dim3(NUM_WARPS*WARP_SIZE) >>> (partialHistograms, (uint*)dataDev, n);\n",
        "\t\n",
        "\t//объдинить гистограммы отдельных блоков вместе\n",
        "\tmergeHistogramKernel <<<dim3(NUM_BINS), dim3(256) >>> (histogram, partialHistograms, numPartials);\n",
        "\t\n",
        "\t// освободить выделенную память\n",
        "\tcudaFree(partialHistograms);\n",
        "}\n",
        "//Заполнить массив случайными байтами\n",
        "void randomInt(uint *a, int n, uint *h)\n",
        "{\n",
        "\tstd::random_device rd;\n",
        "\tstd::mt19937 gen(rd());\n",
        "\tstd::normal_distribution<> X(128, 10);\n",
        "\tdouble seconds = 0;\n",
        "\tfor (int i = 0; i < n; i++)\n",
        "\t{\n",
        "\t\tuchar b1 = static_cast<int>(X(gen)) & 0xFF;\n",
        "\t\tuchar b2 = static_cast<int>(X(gen)) & 0xFF;\n",
        "\t\tuchar b3 = static_cast<int>(X(gen)) & 0xFF;\n",
        "\t\tuchar b4 = static_cast<int>(X(gen)) & 0xFF;\n",
        "\n",
        "\t\ta[i] = b1 | (b2 << 8) | (b3 << 16) | (b4 << 24);\n",
        "\n",
        "\n",
        "\t\tclock_t start = clock();\n",
        "\n",
        "\n",
        "\t\th[b1]++;\n",
        "\t\th[b2]++;\n",
        "\t\th[b3]++;\n",
        "\t\th[b4]++;\n",
        "\t\tclock_t end = clock();\n",
        "\t\tseconds += (double)(end - start) / CLOCKS_PER_SEC;\n",
        "\n",
        "\t}\n",
        "\tprintf(\"CPU time: %.2f ms\\n\", seconds*1000);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tuint *a = new uint[N];\n",
        "\tuint *hDev = NULL;\n",
        "\tuint *aDev = NULL;\n",
        "\tuint h[NUM_BINS];\n",
        "\tuint hHost[NUM_BINS];\n",
        "\tcudaEvent_t start, stop;\n",
        "\tfloat gpuTime = 0.0f;\n",
        "\n",
        "\tmemset(hHost, 0, sizeof(hHost));\n",
        "\trandomInt(a, N, hHost);\n",
        "\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&stop);\n",
        "\tcudaEventRecord(start, 0);\n",
        "\n",
        "\tcudaMalloc((void**)&aDev, N * sizeof(uint));\n",
        "\tcudaMalloc((void**)&hDev, NUM_BINS * sizeof(uint));\n",
        "\tcudaMemcpy(aDev, a, N * sizeof(uint), cudaMemcpyHostToDevice);\n",
        "\n",
        "\thistogram(hDev, aDev, N * 4);\n",
        "\n",
        "\tcudaMemcpy(h, hDev, NUM_BINS * sizeof(uint), cudaMemcpyDeviceToHost);\n",
        "\tcudaFree(aDev);\n",
        "\tcudaFree(hDev);\n",
        "\n",
        "\tcudaEventRecord(stop, 0);\n",
        "\tcudaEventSynchronize(stop);\n",
        "\tcudaEventElapsedTime(&gpuTime, start, stop);\n",
        "\n",
        "\tprintf(\"GPU time: %.2f ms\\n\", gpuTime);\n",
        "\tfor (int i = 0; i < NUM_BINS; i++)\n",
        "\t{\n",
        "\t\t//if (h[i] != hHost[i])\n",
        "\t\t\tprintf(\"Diff at %d-%d, %d\\n\",i,h[i],hHost[i]);\n",
        "\t}\n",
        "\tdelete a;\n",
        "\t\n",
        "\n",
        "\tsystem(\"pause\");\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "uM93FXVbmlNl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}