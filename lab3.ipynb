{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBgDTyUMk6LyWpvpdRFAiT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nastya880/cuda/blob/main/lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Лабораторная 3**"
      ],
      "metadata": {
        "id": "nHEKBKXxllKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гистограмма\n",
        "\n",
        "Дан массив А из N натуральных элементов от 0 до 255 с нормальным распределением. Построить гистограмму, содержащую число каждого элемента массива."
      ],
      "metadata": {
        "id": "09KrS1rnlwJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile histogram.cu\n",
        "#include <cassert>\n",
        "#include <cstring>\n",
        "#include <random>\n",
        "#include <cstdio>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "typedef unsigned int uint;\n",
        "typedef unsigned char uchar;\n",
        "constexpr auto n = (1024 * 2); \n",
        "constexpr auto log2_warp_size = 5;\n",
        "constexpr auto warp_size = 32; \n",
        "constexpr auto tag_mask = 0x07FFFFFFU; \n",
        "constexpr auto num_bins = 256; \n",
        "constexpr auto num_warps = 6; \n",
        "constexpr auto merge_threadblock_size = 256;\n",
        "\n",
        "inline __device__ void add_byte(volatile uint* warp_hist, const uint data, uint thread_tag)\n",
        "{\n",
        "\tuint count;\n",
        "\tdo\n",
        "\t{// прочесть текущее значение счетчика и снять идентификатор нити \n",
        "\t\tcount = warp_hist[data] & tag_mask;\n",
        "\t\tcount = thread_tag | (count + 1);// увеличить его на единицу и поставить свой идентификатор\n",
        "\t\twarp_hist[data] = count;//осуществить запись\n",
        "\t} while (warp_hist[data] != count);//пока запись не прошла успешно\n",
        "}\n",
        "\n",
        "inline __device__ void add_word(volatile uint* warp_hist, const uint data, const uint tag)\n",
        "{\n",
        "\tadd_byte(warp_hist, (data >> 0) & 0xFFU, tag);\n",
        "\tadd_byte(warp_hist, (data >> 8) & 0xFFU, tag);\n",
        "\tadd_byte(warp_hist, (data >> 16) & 0xFFU, tag);\n",
        "\tadd_byte(warp_hist, (data >> 24) & 0xFFU, tag);\n",
        "}\n",
        "\n",
        "__global__ void histogram_kernel(uint* partial_histograms, const uint* data, const uint data_count)\n",
        "{\n",
        "\t__shared__ uint hist[num_bins * num_warps];\n",
        "\tuint* warpHist = hist + (threadIdx.x >> log2_warp_size) * num_bins;\n",
        "\n",
        "#pragma unroll\n",
        "\tfor (uint i = 0; i < num_bins / warp_size; i++)\n",
        "\t\thist[threadIdx.x + i * num_warps * warp_size/*число нитей в блоке=192*/] = 0;\n",
        "\n",
        "\tuint tag = threadIdx.x << (32 - log2_warp_size);//айди нити\n",
        "\t__syncthreads();\n",
        "\n",
        "\n",
        "\tfor (uint pos = blockIdx.x * blockDim.x + threadIdx.x; pos < data_count;\n",
        "\t\tpos += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t\tuint d = data[pos];\n",
        "\t\tadd_word(warpHist, d, tag);\n",
        "\t}\n",
        "\t__syncthreads();\n",
        "\t// объединить гистограммы данного блока и записать результат в глобальную память\n",
        "\t// 192 нити суммируют данные до 256 элементов гистограмм\n",
        "\n",
        "\tfor (uint bin = threadIdx.x; bin < num_bins; bin += num_warps * warp_size)\n",
        "\t{\n",
        "\t\tuint sum = 0;\n",
        "\t\tfor (uint i = 0; i < num_warps; i++)\n",
        "\t\t\tsum += hist[bin + i * num_bins] & tag_mask;\n",
        "\t\tpartial_histograms[blockIdx.x * num_bins + bin] = sum;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void merge_histogram_kernel(uint* out_histogram, const uint* partial_histograms, const uint histogram_count)\n",
        "{\n",
        "\tuint sum = 0;\n",
        "\tfor (uint i = threadIdx.x; i < histogram_count; i += 256)\n",
        "\t\tsum += partial_histograms[blockIdx.x + i * num_bins];\n",
        "\t__shared__ uint data[num_bins];\n",
        "\tdata[threadIdx.x] = sum;\n",
        "\tfor (uint stride = num_bins / 2; stride > 0; stride >>= 1)\n",
        "\t{\n",
        "\t\t__syncthreads();\n",
        "\t\tif (threadIdx.x < stride)\n",
        "\t\t\tdata[threadIdx.x] += data[threadIdx.x + stride];\n",
        "\t}\n",
        "\tif (threadIdx.x == 0)\n",
        "\t\tout_histogram[blockIdx.x] = data[0];\n",
        "}\n",
        "\n",
        "void histogram(uint* histogram, void* data_dev, const uint byteCount)\n",
        "{\n",
        "\tassert(byteCount % 4 == 0);\n",
        "\tconst int n = byteCount / 4;\n",
        "\tint numBlocks = n / (num_warps * warp_size);\n",
        "\tconstexpr int numPartials = 240;\n",
        "\tuint* partialHistograms = nullptr;\n",
        "\tcudaMalloc((void**)&partialHistograms, numPartials * num_bins * sizeof(uint));\n",
        "\thistogram_kernel << <dim3(numPartials), dim3(num_warps * warp_size) >> > (\n",
        "\t\tpartialHistograms, (uint*)data_dev, n);\n",
        "\tmerge_histogram_kernel << <dim3(num_bins), dim3(256) >> > (histogram,\n",
        "\t\tpartialHistograms, numPartials);\n",
        "\tcudaFree(partialHistograms);\n",
        "}\n",
        "\n",
        "void randomInit(uint* a, int n, uint* h)\n",
        "{\n",
        "\tstd::mt19937 gen(1607);\n",
        "\tstd::normal_distribution<> distr(128, 32);\n",
        "\n",
        "\tfor (int i = 0; i < n; i++)\n",
        "\t{\n",
        "\t\tconst uchar b1 = static_cast<int>(distr(gen)) & 0xFF;\n",
        "\t\tconst uchar b2 = static_cast<int>(distr(gen)) & 0xFF;\n",
        "\t\tconst uchar b3 = static_cast<int>(distr(gen)) & 0xFF;\n",
        "\t\tconst uchar b4 = static_cast<int>(distr(gen)) & 0xFF;\n",
        "\t\ta[i] = b1 | (b2 << 8) | (b3 << 16) | (b4 << 24);\n",
        "\t\th[b1]++;\n",
        "\t\th[b2]++;\n",
        "\t\th[b3]++;\n",
        "\t\th[b4]++;\n",
        "\t}\n",
        "}\n",
        "int main(int argc, char* argv[])\n",
        "{\n",
        "\tconst auto a = new uint[n];\n",
        "\tuint* h_dev = nullptr;\n",
        "\tuint* a_dev = nullptr;\n",
        "\tuint h[num_bins];\n",
        "\tuint h_host[num_bins];\n",
        "\tcudaEvent_t start, stop;\n",
        "\tfloat gpu_time = 0.0f;\n",
        "\tmemset(h_host, 0, sizeof(h_host));\n",
        "\trandomInit(a, n, h_host);\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&stop);\n",
        "\tcudaEventRecord(start, nullptr);\n",
        "\tcudaMalloc(reinterpret_cast<void**>(&a_dev), n * sizeof(uint));\n",
        "\tcudaMalloc(reinterpret_cast<void**>(&h_dev), num_bins * sizeof(uint));\n",
        "\tcudaMemcpy(a_dev, a, n * sizeof(uint), cudaMemcpyHostToDevice);\n",
        "\thistogram(h_dev, a_dev, 4 * n);\n",
        "\tcudaMemcpy(h, h_dev, num_bins * sizeof(uint), cudaMemcpyDeviceToHost);\n",
        "\tcudaFree(a_dev);\n",
        "\tcudaFree(h_dev);\n",
        "\tcudaEventRecord(stop, 0);\n",
        "\tcudaEventSynchronize(stop);\n",
        "\tcudaEventElapsedTime(&gpu_time, start, stop);\n",
        "\tprintf(\"Elapsed time: %.2f\\n\", gpu_time);\n",
        "\tfor (int i = 0; i < num_bins; i++)\n",
        "\t{\n",
        "\t\tfor (int j = 0; j < h[i]; j++)\n",
        "\t\t{\n",
        "\t\t\tprintf(\"*\");\n",
        "\t\t}\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "\tdelete[] a;\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "50kKGnd2lvH2",
        "outputId": "796a626c-078b-47f3-9e13-0f22e64c9693",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing histogram.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc histogram.cu -o histogram -Wno-deprecated-gpu-targets\n",
        "!nvprof ./histogram"
      ],
      "metadata": {
        "id": "qMtj9-Wml3-a",
        "outputId": "77ea7823-d2da-47a4-93c2-54e3261cb19a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "histogram.cu(90): warning: variable \"numBlocks\" was declared but never referenced\n",
            "\n",
            "histogram.cu(16): warning: variable \"merge_threadblock_size\" was declared but never referenced\n",
            "\n",
            "==567== NVPROF is profiling process 567, command: ./histogram\n",
            "Elapsed time: 0.49\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*\n",
            "*\n",
            "*\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*\n",
            "**\n",
            "\n",
            "\n",
            "*\n",
            "\n",
            "\n",
            "\n",
            "*\n",
            "*\n",
            "**\n",
            "\n",
            "**\n",
            "*\n",
            "*\n",
            "***\n",
            "****\n",
            "****\n",
            "*\n",
            "*\n",
            "******\n",
            "****\n",
            "**\n",
            "\n",
            "****\n",
            "****\n",
            "**\n",
            "**\n",
            "******\n",
            "*******\n",
            "**********\n",
            "*****\n",
            "*********\n",
            "********\n",
            "***********\n",
            "*****************\n",
            "**********\n",
            "************\n",
            "*****************\n",
            "********\n",
            "***********\n",
            "**************\n",
            "***************\n",
            "******************\n",
            "****************\n",
            "***************\n",
            "**********************\n",
            "*************\n",
            "************************\n",
            "****************************\n",
            "****************************\n",
            "*********************************\n",
            "********************\n",
            "********************************\n",
            "*******************\n",
            "**************************************\n",
            "******************************\n",
            "***********************************\n",
            "*************************************\n",
            "*************************************\n",
            "*****************************************************\n",
            "***************************************\n",
            "*************************************************\n",
            "*********************************************\n",
            "************************************************\n",
            "************************************************\n",
            "****************************************************\n",
            "***************************************************************\n",
            "****************************************************\n",
            "*****************************************************\n",
            "*********************************************************\n",
            "***************************************************\n",
            "***************************************************************\n",
            "***************************************************************\n",
            "****************************************************************\n",
            "*********************************************************************\n",
            "****************************************************************************\n",
            "*******************************************************************\n",
            "***************************************************************************\n",
            "****************************************************************************\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "********************************************************************\n",
            "********************************************************************************\n",
            "*********************************************************************************\n",
            "**************************************************************************************\n",
            "***********************************************************************************************\n",
            "********************************************************************************\n",
            "**********************************************************************************************\n",
            "**********************************************************************************************\n",
            "*********************************************************************************\n",
            "**************************************************************************************************\n",
            "*********************************************************************************************\n",
            "********************************************************************************************\n",
            "**************************************************************************************\n",
            "****************************************************************************\n",
            "*****************************************************************************************************\n",
            "*************************************************************************************************************\n",
            "***************************************************************************************\n",
            "*************************************************************************************\n",
            "************************************************************************************\n",
            "**********************************************************************************************************\n",
            "*********************************************************************************************************\n",
            "*********************************************************************************************************\n",
            "**********************************************************************************************\n",
            "********************************************************************************************************\n",
            "************************************************************************************************************\n",
            "**************************************************************************************\n",
            "***********************************************************************************************************************************\n",
            "***************************************************************************************\n",
            "******************************************************************************************\n",
            "*********************************************************************************************\n",
            "**********************************************************************************************************\n",
            "*********************************************************************************\n",
            "**************************************************************************************\n",
            "*******************************************************************************\n",
            "**************************************************************************************************\n",
            "***************************************************************************************************************\n",
            "******************************************************************************************\n",
            "**************************************************************************************\n",
            "********************************************************************************************\n",
            "**********************************************************************************************************\n",
            "*************************************************************************************\n",
            "******************************************************************************************************\n",
            "**********************************************************************************************\n",
            "*****************************************************************************************\n",
            "********************************************************************\n",
            "**********************************************************************************\n",
            "**********************************************************************\n",
            "**************************************************************************\n",
            "***************************************************************************\n",
            "***********************************************************************\n",
            "*****************************************************************************\n",
            "***********************************************************\n",
            "***************************************************************************\n",
            "*************************************************************\n",
            "*****************************************************************\n",
            "*************************************************************************\n",
            "*********************************************************************\n",
            "***********************************************************************************\n",
            "********************************************************\n",
            "********************************************\n",
            "*************************************\n",
            "**************************************************\n",
            "*******************************************\n",
            "*******************************************************\n",
            "*********************************\n",
            "*************************************\n",
            "*********************************************\n",
            "********************************************\n",
            "*******************************\n",
            "***********************************************\n",
            "**************************\n",
            "*************************************\n",
            "*****************************\n",
            "*********************************\n",
            "**********************************\n",
            "*************************************\n",
            "******************************\n",
            "**************************\n",
            "**********************\n",
            "*********************\n",
            "*****************\n",
            "*****************\n",
            "****************************\n",
            "*****************\n",
            "************\n",
            "***************\n",
            "********************\n",
            "*****************\n",
            "****\n",
            "**************\n",
            "**************\n",
            "********\n",
            "***********\n",
            "***************\n",
            "*\n",
            "**********\n",
            "**********\n",
            "****\n",
            "****\n",
            "****\n",
            "***\n",
            "*****\n",
            "***\n",
            "****\n",
            "***\n",
            "*\n",
            "****\n",
            "****\n",
            "****\n",
            "*****\n",
            "**\n",
            "*\n",
            "***\n",
            "\n",
            "***\n",
            "*\n",
            "*\n",
            "****\n",
            "***\n",
            "\n",
            "*\n",
            "*\n",
            "\n",
            "***\n",
            "\n",
            "*\n",
            "\n",
            "\n",
            "\n",
            "*\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*\n",
            "\n",
            "*\n",
            "\n",
            "\n",
            "*\n",
            "*\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*\n",
            "\n",
            "*\n",
            "\n",
            "==567== Profiling application: ./histogram\n",
            "==567== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   53.48%  16.480us         1  16.480us  16.480us  16.480us  merge_histogram_kernel(unsigned int*, unsigned int const *, unsigned int)\n",
            "                   32.09%  9.8880us         1  9.8880us  9.8880us  9.8880us  histogram_kernel(unsigned int*, unsigned int const *, unsigned int)\n",
            "                    7.58%  2.3360us         1  2.3360us  2.3360us  2.3360us  [CUDA memcpy HtoD]\n",
            "                    6.85%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.40%  504.97ms         2  252.49ms  1.0970us  504.97ms  cudaEventCreate\n",
            "                    0.20%  1.0239ms         1  1.0239ms  1.0239ms  1.0239ms  cudaEventSynchronize\n",
            "                    0.16%  814.23us         1  814.23us  814.23us  814.23us  cuDeviceGetPCIBusId\n",
            "                    0.10%  492.97us         1  492.97us  492.97us  492.97us  cuDeviceTotalMem\n",
            "                    0.05%  242.46us         3  80.819us  3.2940us  234.41us  cudaMalloc\n",
            "                    0.03%  166.40us       101  1.6470us     153ns  71.165us  cuDeviceGetAttribute\n",
            "                    0.03%  142.08us         3  47.359us  3.8290us  114.21us  cudaFree\n",
            "                    0.01%  51.328us         1  51.328us  51.328us  51.328us  cuDeviceGetName\n",
            "                    0.01%  46.381us         2  23.190us  20.072us  26.309us  cudaMemcpy\n",
            "                    0.01%  43.352us         2  21.676us  7.7440us  35.608us  cudaLaunchKernel\n",
            "                    0.00%  14.330us         2  7.1650us  4.1040us  10.226us  cudaEventRecord\n",
            "                    0.00%  3.5470us         1  3.5470us  3.5470us  3.5470us  cudaEventElapsedTime\n",
            "                    0.00%  2.0940us         3     698ns     261ns  1.3620us  cuDeviceGetCount\n",
            "                    0.00%  1.8010us         2     900ns     231ns  1.5700us  cuDeviceGet\n",
            "                    0.00%     327ns         1     327ns     327ns     327ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Редукция\n",
        "\n",
        "Дан массив А из N элементов (задаются случайно). Выполнить редукцию массива А с базовой операцией min."
      ],
      "metadata": {
        "id": "Hn47pOq3mNB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "#include <cstdlib>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        "#include <ctime>\n",
        "\n",
        "constexpr auto block_size = 256;\n",
        "\n",
        "__device__ int min_g(const int a, const int b)\n",
        "{\n",
        "\tif(a==0 || b==0)\n",
        "\t{\n",
        "\t\treturn INT32_MAX;\n",
        "\t}\n",
        "\n",
        "\tif (a < b)\n",
        "\t{\n",
        "\t\treturn a;\n",
        "\t}\n",
        "\treturn b;\n",
        "}\n",
        "\n",
        "__global__ void reduce5(const int* in_data, int* out_data)\n",
        "{\n",
        "\t__shared__ int data[block_size];\n",
        "\tconst int tid = static_cast<int>(threadIdx.x);\n",
        "\tconst int i = static_cast<int>(2 * blockIdx.x * blockDim.x + threadIdx.x);\n",
        "\tdata[tid] = min_g(in_data[i], in_data[i + blockDim.x]);\n",
        "\t__syncthreads();\n",
        "\tfor (int s = static_cast<int>(blockDim.x) / 2; s > 32; s >>= 1)\n",
        "\t{\n",
        "\t\tif (tid < s)\n",
        "\t\t\tdata[tid] = min_g(data[tid], data[tid + s]);\n",
        "\t\t__syncthreads();\n",
        "\t}\n",
        "\tif (tid < 32)\n",
        "\t{\n",
        "\t\tdata[tid] = min_g(data[tid], data[tid + 32]);\n",
        "\t\tdata[tid] = min_g(data[tid], data[tid + 16]);\n",
        "\t\tdata[tid] = min_g(data[tid], data[tid + 8]);\n",
        "\t\tdata[tid] = min_g(data[tid], data[tid + 4]);\n",
        "\t\tdata[tid] = min_g(data[tid], data[tid + 2]);\n",
        "\t\tdata[tid] = min_g(data[tid], data[tid + 1]);\n",
        "\t}\n",
        "\tif (tid == 0)\n",
        "\t\tout_data[blockIdx.x] = data[0];\n",
        "}\n",
        "\n",
        "int reduce(const int* data, const int n)\n",
        "{\n",
        "\tconst auto matrix_size = n * sizeof(int);\n",
        "\n",
        "\tint* sums = nullptr;\n",
        "\tint* data_cuda = nullptr;\n",
        "\n",
        "\tconst int num_blocks = n / block_size;\n",
        "\tint res = INT32_MAX;\n",
        "\n",
        "\tcudaMalloc(reinterpret_cast<void**>(&data_cuda), matrix_size);\n",
        "\tcudaMalloc(reinterpret_cast<void**>(&sums), matrix_size);\n",
        "\tcudaMemcpy(data_cuda, data, matrix_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\treduce5 <<< dim3(num_blocks), dim3(block_size) >>> (data_cuda, sums);\n",
        "\n",
        "\tif (num_blocks > block_size)\n",
        "\t{\n",
        "\t\tres = reduce(sums, num_blocks);\n",
        "\t}\n",
        "\telse\n",
        "\t{\n",
        "\t\tconst auto sums_host = new int[num_blocks];\n",
        "\t\tcudaMemcpy(sums_host, sums, num_blocks * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\t\tfor (int i = 0; i < num_blocks; i++)\n",
        "\t\t{\n",
        "\t\t\tres = res < sums_host[i] ? res : sums_host[i];//проверяем оставшиеся части\n",
        "\t\t}\n",
        "\t\tdelete[] sums_host;\n",
        "\t}\n",
        "\tcudaFree(sums);\n",
        "\treturn res;\n",
        "}\n",
        "\n",
        "bool check_min(const int* a, const int actual_min, const int n)\n",
        "{\n",
        "\tauto expected_min = a[0];\n",
        "\tfor (auto i = 0; i < n; i++)\n",
        "\t{\n",
        "\t\tif (expected_min > a[i])\n",
        "\t\t{\n",
        "\t\t\texpected_min = a[i];\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tstd::cout << \"Expected min: \" << expected_min << std::endl;\n",
        "\n",
        "\treturn expected_min == actual_min;\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[])\n",
        "{\n",
        "\tsrand(time(nullptr));\n",
        "\tconstexpr auto matrix_len = 1024 * 1024;\n",
        "\tconstexpr auto matrix_size = static_cast<int>(matrix_len) * sizeof(int);\n",
        "\tconst auto a = static_cast<int*>(malloc(matrix_size));\n",
        "\tfor (int i = 0; i < matrix_len; i++)\n",
        "\t{\n",
        "\t\ta[i] = rand() % matrix_len + block_size;\n",
        "\t}\n",
        "\n",
        "\tauto actual_min = reduce(a, matrix_len);\n",
        "\tstd::cout << \"Actual min_g = \" << actual_min << \". Result is right: \" << (check_min(a, actual_min, matrix_len) == 1 ? \"true\" : \"false\");\n",
        "\n",
        "\tfree(a);\n",
        "\n",
        "\treturn EXIT_SUCCESS;\n",
        "}\n"
      ],
      "metadata": {
        "id": "uM93FXVbmlNl",
        "outputId": "57d12536-0773-4ab1-eecd-42d0cb91c345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -Wno-deprecated-gpu-targets\n",
        "!nvprof ./reduction"
      ],
      "metadata": {
        "id": "J08MJMFrYqNf",
        "outputId": "ba716ed3-c622-4d1a-8a6a-4b0f5cdf29ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==699== NVPROF is profiling process 699, command: ./reduction\n",
            "Actual min_g = 260. Result is right: Expected min: 256\n",
            "false==699== Profiling application: ./reduction\n",
            "==699== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   91.61%  850.11us         1  850.11us  850.11us  850.11us  [CUDA memcpy HtoD]\n",
            "                    7.64%  70.912us         2  35.456us  5.6000us  65.312us  reduce5(int const *, int*)\n",
            "                    0.53%  4.8960us         1  4.8960us  4.8960us  4.8960us  [CUDA memcpy DtoD]\n",
            "                    0.22%  2.0800us         1  2.0800us  2.0800us  2.0800us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.56%  436.35ms         4  109.09ms  3.8720us  436.05ms  cudaMalloc\n",
            "                    0.25%  1.0989ms         3  366.30us  20.179us  1.0542ms  cudaMemcpy\n",
            "                    0.10%  443.15us         1  443.15us  443.15us  443.15us  cuDeviceTotalMem\n",
            "                    0.04%  164.48us       101  1.6280us     160ns  71.429us  cuDeviceGetAttribute\n",
            "                    0.02%  102.54us         2  51.268us  6.4510us  96.085us  cudaFree\n",
            "                    0.01%  56.043us         2  28.021us  14.545us  41.498us  cudaLaunchKernel\n",
            "                    0.01%  38.977us         1  38.977us  38.977us  38.977us  cuDeviceGetName\n",
            "                    0.00%  7.1950us         1  7.1950us  7.1950us  7.1950us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3740us         3     791ns     265ns  1.6800us  cuDeviceGetCount\n",
            "                    0.00%  1.5870us         2     793ns     413ns  1.1740us  cuDeviceGet\n",
            "                    0.00%     364ns         1     364ns     364ns     364ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ]
}